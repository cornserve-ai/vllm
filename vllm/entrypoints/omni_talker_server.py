import queue
import wave
import io
import argparse
from argparse import Namespace
import asyncio
import json
import os
import signal
import uuid
from vllm.entrypoints.openai.protocol import ChatCompletionRequest

from fastapi import FastAPI, Request, Response
import numpy as np
import psutil
import uvicorn

from vllm.engine.arg_utils import nullable_str
from vllm.engine.async_llm_engine import AsyncEngineArgs
from vllm.engine.omni_talker_vocoder_engine import OmniTalkerVocoderEngine
from vllm.logger import init_logger
from vllm.model_executor.layers.quantization import QUANTIZATION_METHODS
from vllm.sampling_params import SamplingParams
from vllm.entrypoints.omni_utils import OmniProcessor

logger = init_logger('vllm.entrypoints.talker_server')

app = FastAPI()

@app.get("/health")
async def health(raw_request: Request) -> Response:
    """Health check."""
    return Response(status_code=200)

@app.post("/v1/chat/completions")
async def generate(request: ChatCompletionRequest, raw_request: Request) -> Response:
    """Generate completion for the request.

    Need to handle sidecar ranks, returns encoded wav audios.
    """
    request.seed = 0
    processor: OmniProcessor = raw_request.app.state.processor

    logger.info("Received request: %s", request)
    prompt = await processor.process(request.messages)
    logger.info("Processed prompt %s", prompt)

    sampling_params = SamplingParams(
        temperature=0.0,
        top_k=-1,
        top_p=1.0,
        repetition_penalty=1.1,
        max_tokens=request.max_tokens,
        detokenize=True,
        seed=request.seed,
    )
    talker_sampling_params = SamplingParams(
        temperature=0.9,
        top_k=40,
        top_p=0.8,
        repetition_penalty=1.05,
        max_tokens=2048,
        detokenize=False,
        seed=request.seed,
    )
    request_id = request.request_id if request.request_id else str(uuid.uuid4().hex)
    output_queue = raw_request.app.state.engine.add_request(
        request_id,
        prompt,
        sampling_params,
        **{
            "talker_params": talker_sampling_params,
        },
        voice_type=raw_request.app.state.voice_type,
    )
    waveforms = []
    async def _await_next_chunk(q: queue.Queue):
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, q.get)

    while True:
        output = await _await_next_chunk(output_queue)
        if output is None:
            break
        output_wav, _ = output
        waveforms.append(output_wav)

    # all_waves = np.concatenate(waveforms)
    # return Response(content=all_waves.tobytes())

    # Below generated by GPT, CHECKME
    # 1. Concatenate all chunks
    all_waves = np.concatenate(waveforms, axis=0)

    # 2. Convert to little-endian 16-bit PCM for WAV
    pcm16 = (np.clip(all_waves, -1.0, 1.0) * 32767).astype("<i2")

    # 3. Wrap in a RIFF/WAVE container
    buffer = io.BytesIO()
    with wave.open(buffer, "wb") as wf:
        wf.setnchannels(1)        # mono; change to 2 for stereo
        wf.setsampwidth(2)        # 2 bytes = 16 bits
        wf.setframerate(24_000)   # match TTS sample-rate
        wf.writeframes(pcm16.tobytes())
    buffer.seek(0)

    # 4. Return a proper WAV response
    # TODO: streaming
    return Response(
        content=buffer.read(),
        media_type="audio/wav",
        headers={"Content-Disposition": 'attachment; filename="speech.wav"'},
    )

def init_omni_engine(args: Namespace) -> OmniTalkerVocoderEngine:
    model_name = "Qwen/Qwen2.5-Omni-7B"
    # only used for loading thinker embeddings
    thinker_engine_args = AsyncEngineArgs(model=model_name, trust_remote_code=True)
    talker_engine_args = AsyncEngineArgs(
        model=model_name,
        trust_remote_code=True,
        gpu_memory_utilization=args.gpu_memory_utilization,
        tensor_parallel_size=1,
        enforce_eager=args.enforce_eager,
        distributed_executor_backend="mp",
        limit_mm_per_prompt={
            'audio': 32,
            'image': 960,
            'video': 32
        },
        max_model_len=32768,
        max_num_seqs=args.max_num_seqs,
        block_size=args.block_size,
        quantization=args.talker_quantization,
        enable_prefix_caching=args.enable_prefix_caching,
        cornserve_sidecar_ranks=args.cornserve_sidecar_ranks,
    )

    return OmniTalkerVocoderEngine(
        thinker_engine_args,
        talker_engine_args,
        model_name,
        code2wav_enable_torch_compile=args.enable_torch_compile,
        code2wav_enable_torch_compile_first_chunk=args.
        enable_torch_compile_first_chunk,
        code2wav_odeint_method=args.odeint_method,
        code2wav_odeint_method_relaxed=args.odeint_method_relaxed,
        code2wav_batched_chunk=args.batched_chunk,
        code2wav_frequency=args.code2wav_frequency,
        talker_visible_devices=args.talker_devices,
        code2wav_visible_devices=args.code2wav_devices,
        code2wav_dynamic_batch=True,
    )

def init_app(
    args: Namespace,
) -> FastAPI:
    app.state.engine = init_omni_engine(args)
    app.state.seed = args.seed
    # TODO: per request voice type
    app.state.voice_type = args.voice_type
    app.state.processor = OmniProcessor()
    return app

async def run_server(args: Namespace) -> None:
    """Serve the Eric model as a FastAPI app."""
    logger.info("Starting TalkerVocder Engine with %s", args)

    # if args.cornserve_sidecar_ranks:
    #     configure_otel(f"eric{str(eric_config.sidecar.ranks).replace(' ', '')}")

    global app
    app = init_app(args)

    # FastAPIInstrumentor().instrument_app(app)
    # ThreadingInstrumentor().instrument()

    logger.info("Available routes are:")
    for route in app.routes:
        methods = getattr(route, "methods", None)
        path = getattr(route, "path", None)

        if methods is None or path is None:
            continue

        logger.info(
            "%s %s",
            list(methods)[0] if len(methods) == 1 else "{" + ",".join(methods) + "}",
            path,
        )

    config = uvicorn.Config(app, host=args.host, port=args.port)
    server = uvicorn.Server(config)

    loop = asyncio.get_running_loop()
    server_task = loop.create_task(server.serve())

    def shutdown() -> None:
        engine: OmniTalkerVocoderEngine = app.state.engine
        engine.shutdown()
        current_process = psutil.Process()
        children = current_process.children(recursive=True)
        for child in children:
            os.kill(child.pid, signal.SIGTERM)
        server_task.cancel()

    loop.add_signal_handler(signal.SIGINT, shutdown)
    loop.add_signal_handler(signal.SIGTERM, shutdown)

    try:
        await server_task
    except asyncio.CancelledError:
        logger.info("Shutting down FastAPI server.")
        await server.shutdown()

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    # TODO: make this config
    parser.add_argument("--host", type=str, default=None)
    parser.add_argument("--port", type=int, default=8000)
    parser.add_argument('--enforce-eager', action='store_true')
    parser.add_argument('--block-size', type=int, default=16)
    parser.add_argument("--log-level", type=str, default="debug")
    parser.add_argument("--seed", type=int, default=0)
    parser.add_argument('--enable-prefix-caching', action='store_true')
    parser.add_argument('--talker-quantization',
                        type=nullable_str,
                        choices=QUANTIZATION_METHODS)
    parser.add_argument('--gpu-memory-utilization', type=float, default=0.6)
    parser.add_argument('--max-num-seqs', type=int, default=64)
    parser.add_argument('--enable-torch-compile', action='store_true')
    parser.add_argument('--enable-torch-compile-first-chunk', action='store_true')
    parser.add_argument("--odeint-method",
                        type=str,
                        default="rk4",
                        choices=["euler", "rk4"])
    parser.add_argument("--odeint-method-relaxed", action="store_true")
    parser.add_argument("--code2wav-frequency",
                        type=str,
                        default='50hz',
                        choices=['50hz'])
    parser.add_argument('--voice-type', type=nullable_str, default='m02')
    parser.add_argument("--batched-chunk", type=int, default=None)
    parser.add_argument('--talker-devices', type=json.loads, default="[0]")
    parser.add_argument('--code2wav-devices', type=json.loads, default="[0]")

    parser.add_argument(
        "--cornserve-sidecar-ranks",
        type=int,
        nargs="+",
        default = [],
        help = "List of integer ranks for sidecar servers."
    )

    args = parser.parse_args()

    asyncio.run(run_server(args))
